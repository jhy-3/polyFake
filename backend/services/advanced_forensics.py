"""
PolySleuth - é«˜çº§åˆ·é‡æ£€æµ‹ä¸å¸‚åœºæ“çºµåˆ†æ

åŒ…å«ï¼š
1. è‡ªäº¤æ˜“(åˆ·é‡)æ£€æµ‹
2. å¾ªç¯äº¤æ˜“æ£€æµ‹ï¼ˆå›¾ç®—æ³•ï¼‰
3. åŸå­åŒ–åˆ·é‡æ¨¡å¼æ£€æµ‹
4. äº¤æ˜“é‡å¼‚å¸¸æ£€æµ‹
5. å¥³å·«æ”»å‡»é›†ç¾¤æ£€æµ‹
6. ç»¼åˆå¸‚åœºå¥åº·è¯„ä¼°
"""
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple, Set
from dataclasses import dataclass, field
from collections import defaultdict
from decimal import Decimal
import hashlib

import pandas as pd
import numpy as np

try:
    import networkx as nx
    HAS_NETWORKX = True
except ImportError:
    HAS_NETWORKX = False
    logging.warning("NetworkX not installed. Circular trade detection will be limited.")

from ..models import SessionLocal, TradeDB

logger = logging.getLogger(__name__)


# ============================================================================
# æ•°æ®ç»“æ„å®šä¹‰
# ============================================================================

@dataclass
class WashTradeEvidence:
    """åˆ·é‡äº¤æ˜“è¯æ®"""
    evidence_type: str  # SELF_TRADE, CIRCULAR, ATOMIC, VOLUME_SPIKE, SYBIL_CLUSTER
    tx_hash: str
    addresses: List[str]
    confidence: float
    volume: float
    details: Dict
    timestamp: Optional[datetime] = None


@dataclass
class CircularPath:
    """å¾ªç¯äº¤æ˜“è·¯å¾„"""
    path: List[str]  # åœ°å€åˆ—è¡¨
    tx_hashes: List[str]
    total_volume: float
    time_span_minutes: float
    confidence: float


@dataclass
class SybilCluster:
    """å¥³å·«æ”»å‡»é›†ç¾¤"""
    cluster_id: str
    addresses: List[str]
    market_id: str
    side: str  # YES/NO or BUY/SELL
    trade_count: int
    total_volume: float
    win_rate: float
    time_window_seconds: float
    confidence: float


@dataclass
class VolumeSpike:
    """äº¤æ˜“é‡å¼‚å¸¸"""
    market_id: str
    timestamp: datetime
    spike_volume: float
    baseline_volume: float
    spike_ratio: float
    trade_count: int
    is_correlated_with_event: bool = False
    event_info: Optional[str] = None


@dataclass
class MarketHealthReport:
    """å¸‚åœºå¥åº·æŠ¥å‘Š"""
    market_id: str
    health_score: float  # 0-100
    risk_level: str  # LOW, MEDIUM, HIGH, CRITICAL
    total_trades: int
    total_volume: float
    wash_trade_ratio: float
    evidence_list: List[WashTradeEvidence]
    summary: Dict


# ============================================================================
# 1. è‡ªäº¤æ˜“(åˆ·é‡)æ£€æµ‹
# ============================================================================

def detect_self_trades(trades_df: pd.DataFrame) -> List[WashTradeEvidence]:
    """
    æ£€æµ‹è‡ªäº¤æ˜“ï¼ˆåˆ·é‡äº¤æ˜“ï¼‰
    
    æ£€æµ‹é€»è¾‘ï¼š
    1. maker_address == taker_address çš„ç›´æ¥è‡ªäº¤æ˜“
    2. ç›¸åŒ (amount, price, timestamp) çš„äº¤æ˜“å¯èƒ½æ¥è‡ªåŒä¸€èµ„é‡‘æ¥æº
    
    Args:
        trades_df: äº¤æ˜“ DataFrameï¼Œéœ€åŒ…å« maker, taker, size, price, timestamp, tx_hash
    
    Returns:
        åˆ·é‡äº¤æ˜“è¯æ®åˆ—è¡¨
    """
    if trades_df.empty:
        logger.warning("detect_self_trades: è¾“å…¥ DataFrame ä¸ºç©º")
        return []
    
    logger.info("ğŸ” å¼€å§‹è‡ªäº¤æ˜“æ£€æµ‹...")
    evidence_list: List[WashTradeEvidence] = []
    
    # 1. ç›´æ¥è‡ªäº¤æ˜“æ£€æµ‹ (maker == taker)
    direct_self_trades = trades_df[trades_df['maker'] == trades_df['taker']]
    
    for _, trade in direct_self_trades.iterrows():
        evidence = WashTradeEvidence(
            evidence_type="SELF_TRADE_DIRECT",
            tx_hash=trade['tx_hash'],
            addresses=[trade['maker']],
            confidence=0.99,  # ç›´æ¥è‡ªäº¤æ˜“ç½®ä¿¡åº¦æé«˜
            volume=trade['volume'],
            timestamp=trade['timestamp'],
            details={
                'trade_type': 'direct_self_trade',
                'address': trade['maker'],
                'size': trade['size'],
                'price': trade['price'],
                'token_id': trade.get('token_id', ''),
            }
        )
        evidence_list.append(evidence)
    
    logger.info(f"  âœ“ å‘ç° {len(direct_self_trades)} ç¬”ç›´æ¥è‡ªäº¤æ˜“")
    
    # 2. ç›¸åŒäº¤æ˜“ç‰¹å¾æ£€æµ‹ï¼ˆå¯èƒ½çš„å…³è”è‡ªäº¤æ˜“ï¼‰
    # åˆ›å»ºäº¤æ˜“ç‰¹å¾å“ˆå¸Œ
    trades_df = trades_df.copy()
    trades_df['trade_signature'] = trades_df.apply(
        lambda x: f"{x['size']:.6f}_{x['price']:.6f}_{x['timestamp'].strftime('%Y%m%d%H%M')}",
        axis=1
    )
    
    # æŒ‰ç‰¹å¾åˆ†ç»„
    signature_groups = trades_df.groupby('trade_signature')
    
    suspicious_pairs = 0
    for signature, group in signature_groups:
        if len(group) >= 2:
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸åŒçš„ maker/taker å¯¹
            makers = set(group['maker'].unique())
            takers = set(group['taker'].unique())
            
            # å¦‚æœåŒä¸€ç‰¹å¾çš„äº¤æ˜“æ¶‰åŠå°‘é‡åœ°å€ï¼Œå¯èƒ½æ˜¯å…³è”è´¦æˆ·
            all_addresses = makers | takers
            if len(all_addresses) <= 4 and len(group) >= 2:
                total_vol = group['volume'].sum()
                
                evidence = WashTradeEvidence(
                    evidence_type="SELF_TRADE_COORDINATED",
                    tx_hash=group['tx_hash'].iloc[0],
                    addresses=list(all_addresses),
                    confidence=min(0.8, 0.5 + len(group) * 0.1),
                    volume=total_vol,
                    timestamp=group['timestamp'].iloc[0],
                    details={
                        'trade_type': 'coordinated_self_trade',
                        'trade_count': len(group),
                        'signature': signature,
                        'addresses': list(all_addresses),
                    }
                )
                evidence_list.append(evidence)
                suspicious_pairs += 1
    
    logger.info(f"  âœ“ å‘ç° {suspicious_pairs} ç»„åè°ƒè‡ªäº¤æ˜“")
    logger.info(f"âœ… è‡ªäº¤æ˜“æ£€æµ‹å®Œæˆ: å…± {len(evidence_list)} æ¡è¯æ®")
    
    return evidence_list


# ============================================================================
# 2. å¾ªç¯äº¤æ˜“æ£€æµ‹ï¼ˆå›¾ç®—æ³•ï¼‰
# ============================================================================

def detect_circular_trades(
    trades_df: pd.DataFrame,
    window_minutes: int = 60,
    min_cycle_volume: float = 100.0
) -> List[CircularPath]:
    """
    æ£€æµ‹å¾ªç¯äº¤æ˜“è·¯å¾„
    
    ä½¿ç”¨ NetworkX æ„å»ºèµ„é‡‘æµå‘å›¾ï¼Œæ£€æµ‹ç®€å•å¾ªç¯ï¼š
    - A -> B -> A (äºŒèŠ‚ç‚¹å¾ªç¯)
    - A -> B -> C -> A (ä¸‰èŠ‚ç‚¹å¾ªç¯)
    
    Args:
        trades_df: äº¤æ˜“ DataFrame
        window_minutes: æ—¶é—´çª—å£ï¼ˆåˆ†é’Ÿï¼‰
        min_cycle_volume: æœ€å°å¾ªç¯äº¤æ˜“é‡
    
    Returns:
        å¾ªç¯è·¯å¾„åˆ—è¡¨
    """
    if not HAS_NETWORKX:
        logger.warning("NetworkX æœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œå¾ªç¯äº¤æ˜“æ£€æµ‹")
        return []
    
    if trades_df.empty:
        return []
    
    logger.info(f"ğŸ” å¼€å§‹å¾ªç¯äº¤æ˜“æ£€æµ‹ (çª—å£: {window_minutes}åˆ†é’Ÿ)...")
    
    circular_paths: List[CircularPath] = []
    
    # æŒ‰æ—¶é—´çª—å£åˆ†ç»„
    trades_df = trades_df.copy()
    trades_df['time_window'] = trades_df['timestamp'].dt.floor(f'{window_minutes}min')
    
    for window, window_trades in trades_df.groupby('time_window'):
        if len(window_trades) < 3:
            continue
        
        # æ„å»ºæœ‰å‘å›¾
        G = nx.DiGraph()
        edge_trades = defaultdict(list)  # è®°å½•æ¯æ¡è¾¹å¯¹åº”çš„äº¤æ˜“
        
        for _, trade in window_trades.iterrows():
            maker = trade['maker'].lower()
            taker = trade['taker'].lower()
            volume = trade['volume']
            tx_hash = trade['tx_hash']
            
            # æ·»åŠ è¾¹ï¼ˆèµ„é‡‘ä» taker æµå‘ makerï¼Œå› ä¸º taker ä¹°å…¥ï¼‰
            if trade.get('side', 'BUY') == 'BUY':
                G.add_edge(taker, maker, weight=volume)
                edge_trades[(taker, maker)].append({
                    'tx_hash': tx_hash,
                    'volume': volume,
                    'timestamp': trade['timestamp']
                })
            else:
                G.add_edge(maker, taker, weight=volume)
                edge_trades[(maker, taker)].append({
                    'tx_hash': tx_hash,
                    'volume': volume,
                    'timestamp': trade['timestamp']
                })
        
        # æ£€æµ‹ç®€å•å¾ªç¯
        try:
            cycles = list(nx.simple_cycles(G))
            
            for cycle in cycles:
                if len(cycle) < 2 or len(cycle) > 4:
                    continue
                
                # è®¡ç®—å¾ªç¯æ€»äº¤æ˜“é‡
                cycle_volume = 0
                cycle_tx_hashes = []
                
                for i in range(len(cycle)):
                    from_addr = cycle[i]
                    to_addr = cycle[(i + 1) % len(cycle)]
                    
                    if (from_addr, to_addr) in edge_trades:
                        for tx in edge_trades[(from_addr, to_addr)]:
                            cycle_volume += tx['volume']
                            cycle_tx_hashes.append(tx['tx_hash'])
                
                if cycle_volume >= min_cycle_volume:
                    # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºå¾ªç¯é•¿åº¦å’Œäº¤æ˜“é‡ï¼‰
                    confidence = min(0.95, 0.6 + (cycle_volume / 10000) * 0.1)
                    if len(cycle) == 2:
                        confidence = min(0.98, confidence + 0.1)
                    
                    path = CircularPath(
                        path=cycle,
                        tx_hashes=list(set(cycle_tx_hashes)),
                        total_volume=cycle_volume,
                        time_span_minutes=window_minutes,
                        confidence=confidence
                    )
                    circular_paths.append(path)
        
        except Exception as e:
            logger.debug(f"å¾ªç¯æ£€æµ‹å‡ºé”™: {e}")
    
    # å»é‡ï¼ˆåŸºäºè·¯å¾„ï¼‰
    seen_paths = set()
    unique_paths = []
    for path in circular_paths:
        path_key = tuple(sorted(path.path))
        if path_key not in seen_paths:
            seen_paths.add(path_key)
            unique_paths.append(path)
    
    logger.info(f"âœ… å¾ªç¯äº¤æ˜“æ£€æµ‹å®Œæˆ: å‘ç° {len(unique_paths)} æ¡å¾ªç¯è·¯å¾„")
    return unique_paths


def circular_paths_to_evidence(paths: List[CircularPath]) -> List[WashTradeEvidence]:
    """å°†å¾ªç¯è·¯å¾„è½¬æ¢ä¸ºè¯æ®æ ¼å¼"""
    evidence_list = []
    
    for path in paths:
        evidence = WashTradeEvidence(
            evidence_type="CIRCULAR_TRADE",
            tx_hash=path.tx_hashes[0] if path.tx_hashes else "",
            addresses=path.path,
            confidence=path.confidence,
            volume=path.total_volume,
            details={
                'cycle_path': ' -> '.join(path.path[:4]) + ' -> ' + path.path[0],
                'cycle_length': len(path.path),
                'all_tx_hashes': path.tx_hashes,
                'time_span_minutes': path.time_span_minutes,
            }
        )
        evidence_list.append(evidence)
    
    return evidence_list


# ============================================================================
# 3. åŸå­åŒ–åˆ·é‡æ¨¡å¼æ£€æµ‹ (Split-Trade-Merge)
# ============================================================================

def detect_atomic_wash_patterns(
    trades_df: pd.DataFrame,
    logs_df: Optional[pd.DataFrame] = None
) -> List[WashTradeEvidence]:
    """
    æ£€æµ‹åŸå­åŒ–åˆ·é‡æ¨¡å¼
    
    åœ¨å•ä¸ªäº¤æ˜“ä¸­æ£€æµ‹åºåˆ—ï¼šPositionSplit -> OrderFilled -> PositionsMerge
    å¦‚æœåŒä¸€ç”¨æˆ·åœ¨åŒä¸€äº¤æ˜“ä¸­å®Œæˆäº†æ‹†åˆ†-äº¤æ˜“-åˆå¹¶ï¼Œåˆ™ä¸ºé«˜ç½®ä¿¡åº¦åˆ·é‡
    
    Args:
        trades_df: äº¤æ˜“ DataFrame
        logs_df: æ—¥å¿— DataFrameï¼ˆå¯é€‰ï¼ŒåŒ…å« Split/Merge äº‹ä»¶ï¼‰
    
    Returns:
        åˆ·é‡è¯æ®åˆ—è¡¨
    """
    logger.info("ğŸ” å¼€å§‹åŸå­åŒ–åˆ·é‡æ¨¡å¼æ£€æµ‹...")
    
    evidence_list: List[WashTradeEvidence] = []
    
    # å¦‚æœæ²¡æœ‰æ—¥å¿—æ•°æ®ï¼Œä½¿ç”¨äº¤æ˜“æ•°æ®çš„å¯å‘å¼æ£€æµ‹
    if logs_df is None or logs_df.empty:
        # å¯å‘å¼ï¼šæ£€æµ‹åŒä¸€åŒºå—å†…åŒä¸€åœ°å€çš„å¤šç¬”åå‘äº¤æ˜“
        trades_df = trades_df.copy()
        
        # æŒ‰åŒºå—å’Œåœ°å€åˆ†ç»„
        for (block, address), group in trades_df.groupby(['block_number', 'maker']):
            if len(group) < 2:
                continue
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¹°å–å¯¹å†²
            buys = group[group['side'] == 'BUY']
            sells = group[group['side'] == 'SELL']
            
            if len(buys) > 0 and len(sells) > 0:
                # è®¡ç®—ä¹°å–é‡å·®å¼‚
                buy_volume = buys['volume'].sum()
                sell_volume = sells['volume'].sum()
                
                # å¦‚æœä¹°å–é‡æ¥è¿‘ï¼Œå¯èƒ½æ˜¯åˆ·é‡
                volume_ratio = min(buy_volume, sell_volume) / max(buy_volume, sell_volume) if max(buy_volume, sell_volume) > 0 else 0
                
                if volume_ratio > 0.8:  # ä¹°å–é‡ç›¸å·®ä¸è¶…è¿‡20%
                    confidence = min(0.9, 0.7 + volume_ratio * 0.2)
                    
                    evidence = WashTradeEvidence(
                        evidence_type="ATOMIC_WASH",
                        tx_hash=group['tx_hash'].iloc[0],
                        addresses=[address],
                        confidence=confidence,
                        volume=buy_volume + sell_volume,
                        timestamp=group['timestamp'].iloc[0],
                        details={
                            'pattern': 'buy_sell_hedge',
                            'buy_volume': buy_volume,
                            'sell_volume': sell_volume,
                            'volume_ratio': volume_ratio,
                            'block_number': block,
                            'trade_count': len(group),
                        }
                    )
                    evidence_list.append(evidence)
    
    else:
        # ä½¿ç”¨æ—¥å¿—æ•°æ®è¿›è¡Œç²¾ç¡®æ£€æµ‹
        # æŒ‰äº¤æ˜“å“ˆå¸Œåˆ†ç»„
        for tx_hash, tx_logs in logs_df.groupby('tx_hash'):
            event_types = set(tx_logs['event_type'].unique())
            
            # æ£€æµ‹ Split-Trade-Merge æ¨¡å¼
            has_split = 'PositionSplit' in event_types or 'Split' in event_types
            has_trade = 'OrderFilled' in event_types or 'Trade' in event_types
            has_merge = 'PositionsMerge' in event_types or 'Merge' in event_types
            
            if has_split and has_trade and has_merge:
                # è·å–æ¶‰åŠçš„åœ°å€
                addresses = list(tx_logs['address'].unique()) if 'address' in tx_logs.columns else []
                
                evidence = WashTradeEvidence(
                    evidence_type="ATOMIC_WASH",
                    tx_hash=tx_hash,
                    addresses=addresses,
                    confidence=0.99,  # Split-Trade-Merge æ¨¡å¼ç½®ä¿¡åº¦æé«˜
                    volume=tx_logs['volume'].sum() if 'volume' in tx_logs.columns else 0,
                    details={
                        'pattern': 'split_trade_merge',
                        'events': list(event_types),
                        'log_count': len(tx_logs),
                    }
                )
                evidence_list.append(evidence)
    
    logger.info(f"âœ… åŸå­åŒ–åˆ·é‡æ£€æµ‹å®Œæˆ: å‘ç° {len(evidence_list)} æ¡è¯æ®")
    return evidence_list


# ============================================================================
# 4. äº¤æ˜“é‡å¼‚å¸¸æ£€æµ‹
# ============================================================================

def detect_volume_spikes(
    trades_df: pd.DataFrame,
    threshold: float = 10.0,
    bin_minutes: int = 5,
    baseline_hours: float = 1.0,
    news_timestamps: Optional[List[datetime]] = None
) -> List[VolumeSpike]:
    """
    æ£€æµ‹äº¤æ˜“é‡å¼‚å¸¸
    
    ç›‘æ§æ¯ä¸ªå¸‚åœºçš„ 5 åˆ†é’Ÿäº¤æ˜“é‡ï¼Œæ ‡è®°è¶…è¿‡ 1 å°æ—¶æ»šåŠ¨å¹³å‡ 10 å€çš„æ—¶æ®µ
    
    Args:
        trades_df: äº¤æ˜“ DataFrame
        threshold: å¼‚å¸¸é˜ˆå€¼å€æ•°ï¼ˆé»˜è®¤ 10 å€ï¼‰
        bin_minutes: æ—¶é—´åˆ†ç®±å¤§å°ï¼ˆåˆ†é’Ÿï¼‰
        baseline_hours: åŸºå‡†è®¡ç®—æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰
        news_timestamps: æ–°é—»/äº‹ä»¶æ—¶é—´æˆ³åˆ—è¡¨ï¼ˆç”¨äºå…³è”åˆ†æï¼‰
    
    Returns:
        äº¤æ˜“é‡å¼‚å¸¸åˆ—è¡¨
    """
    if trades_df.empty:
        return []
    
    logger.info(f"ğŸ” å¼€å§‹äº¤æ˜“é‡å¼‚å¸¸æ£€æµ‹ (é˜ˆå€¼: {threshold}x)...")
    
    spikes: List[VolumeSpike] = []
    
    trades_df = trades_df.copy()
    trades_df['time_bin'] = trades_df['timestamp'].dt.floor(f'{bin_minutes}min')
    
    # æŒ‰å¸‚åœºåˆ†ç»„åˆ†æ
    for token_id, market_trades in trades_df.groupby('token_id'):
        # æŒ‰æ—¶é—´åˆ†ç®±è®¡ç®—äº¤æ˜“é‡
        volume_by_bin = market_trades.groupby('time_bin').agg({
            'volume': 'sum',
            'tx_hash': 'count'
        }).rename(columns={'tx_hash': 'trade_count'})
        
        if len(volume_by_bin) < 3:
            continue
        
        # è®¡ç®—æ»šåŠ¨å¹³å‡ï¼ˆ1å°æ—¶çª—å£ï¼‰
        rolling_window = int(baseline_hours * 60 / bin_minutes)
        volume_by_bin['rolling_avg'] = volume_by_bin['volume'].rolling(
            window=rolling_window,
            min_periods=1
        ).mean().shift(1)  # shift é¿å…åŒ…å«å½“å‰ bin
        
        # å¡«å……ç¬¬ä¸€ä¸ªçª—å£
        volume_by_bin['rolling_avg'] = volume_by_bin['rolling_avg'].fillna(
            volume_by_bin['volume'].expanding().mean()
        )
        
        # è®¡ç®—å¼‚å¸¸æ¯”ç‡
        volume_by_bin['spike_ratio'] = volume_by_bin['volume'] / volume_by_bin['rolling_avg'].replace(0, 1)
        
        # ç­›é€‰å¼‚å¸¸
        anomalies = volume_by_bin[volume_by_bin['spike_ratio'] > threshold]
        
        for timestamp, row in anomalies.iterrows():
            # æ£€æŸ¥æ˜¯å¦ä¸æ–°é—»äº‹ä»¶å…³è”
            is_correlated = False
            event_info = None
            
            if news_timestamps:
                for news_ts in news_timestamps:
                    time_diff = abs((timestamp - news_ts).total_seconds())
                    if time_diff < 3600:  # 1å°æ—¶å†…
                        is_correlated = True
                        event_info = f"Event at {news_ts}"
                        break
            
            spike = VolumeSpike(
                market_id=token_id,
                timestamp=timestamp,
                spike_volume=row['volume'],
                baseline_volume=row['rolling_avg'],
                spike_ratio=row['spike_ratio'],
                trade_count=int(row['trade_count']),
                is_correlated_with_event=is_correlated,
                event_info=event_info
            )
            spikes.append(spike)
    
    logger.info(f"âœ… äº¤æ˜“é‡å¼‚å¸¸æ£€æµ‹å®Œæˆ: å‘ç° {len(spikes)} æ¬¡å¼‚å¸¸")
    return spikes


def volume_spikes_to_evidence(spikes: List[VolumeSpike]) -> List[WashTradeEvidence]:
    """å°†äº¤æ˜“é‡å¼‚å¸¸è½¬æ¢ä¸ºè¯æ®æ ¼å¼"""
    evidence_list = []
    
    for spike in spikes:
        # å¦‚æœä¸äº‹ä»¶å…³è”ï¼Œé™ä½ç½®ä¿¡åº¦ï¼ˆå¯èƒ½æ˜¯æ­£å¸¸çš„å¸‚åœºååº”ï¼‰
        confidence = 0.7 if spike.is_correlated_with_event else 0.85
        confidence = min(0.95, confidence + (spike.spike_ratio - 10) * 0.01)
        
        evidence = WashTradeEvidence(
            evidence_type="VOLUME_SPIKE",
            tx_hash="",  # å¤šç¬”äº¤æ˜“
            addresses=[],
            confidence=confidence,
            volume=spike.spike_volume,
            timestamp=spike.timestamp,
            details={
                'market_id': spike.market_id,
                'spike_ratio': spike.spike_ratio,
                'baseline_volume': spike.baseline_volume,
                'trade_count': spike.trade_count,
                'is_correlated_with_event': spike.is_correlated_with_event,
                'event_info': spike.event_info,
            }
        )
        evidence_list.append(evidence)
    
    return evidence_list


# ============================================================================
# 5. å¥³å·«æ”»å‡»é›†ç¾¤æ£€æµ‹ï¼ˆåè°ƒæŠ•æ³¨ï¼‰
# ============================================================================

def detect_coordinated_clusters(
    trades_df: pd.DataFrame,
    time_window_seconds: int = 10,
    min_cluster_size: int = 3,
    size_tolerance: float = 0.2
) -> List[SybilCluster]:
    """
    æ£€æµ‹å¥³å·«æ”»å‡»é›†ç¾¤ï¼ˆåè°ƒæŠ•æ³¨è¡Œä¸ºï¼‰
    
    æ£€æµ‹é€»è¾‘ï¼š
    - åœ¨ 10 ç§’çª—å£å†…
    - å¤šä¸ªé’±åŒ…å¯¹åŒä¸€å¸‚åœº
    - æŠ¼æ³¨ç›¸åŒæ–¹å‘ï¼ˆå…¨æ˜¯ YES æˆ–å…¨æ˜¯ NOï¼‰
    - äº¤æ˜“è§„æ¨¡ç›¸ä¼¼
    
    Args:
        trades_df: äº¤æ˜“ DataFrame
        time_window_seconds: æ—¶é—´çª—å£ï¼ˆç§’ï¼‰
        min_cluster_size: æœ€å°é›†ç¾¤å¤§å°
        size_tolerance: äº¤æ˜“è§„æ¨¡å®¹å·®ï¼ˆ20%ï¼‰
    
    Returns:
        å¥³å·«é›†ç¾¤åˆ—è¡¨
    """
    if trades_df.empty:
        return []
    
    logger.info(f"ğŸ” å¼€å§‹å¥³å·«é›†ç¾¤æ£€æµ‹ (çª—å£: {time_window_seconds}ç§’)...")
    
    clusters: List[SybilCluster] = []
    
    trades_df = trades_df.copy()
    trades_df['timestamp_sec'] = trades_df['timestamp'].astype('int64') // 10**9
    trades_df['time_window'] = (trades_df['timestamp_sec'] // time_window_seconds) * time_window_seconds
    
    # æŒ‰å¸‚åœºã€æ—¶é—´çª—å£ã€æ–¹å‘åˆ†ç»„
    for (token_id, time_window, side), group in trades_df.groupby(['token_id', 'time_window', 'side']):
        if len(group) < min_cluster_size:
            continue
        
        # è·å–å”¯ä¸€åœ°å€
        makers = set(group['maker'].unique())
        takers = set(group['taker'].unique())
        all_addresses = makers | takers
        
        if len(all_addresses) < min_cluster_size:
            continue
        
        # æ£€æŸ¥äº¤æ˜“è§„æ¨¡æ˜¯å¦ç›¸ä¼¼
        sizes = group['size'].values
        mean_size = np.mean(sizes)
        
        if mean_size == 0:
            continue
        
        size_deviations = np.abs(sizes - mean_size) / mean_size
        similar_size_ratio = np.mean(size_deviations < size_tolerance)
        
        if similar_size_ratio < 0.6:  # è‡³å°‘60%çš„äº¤æ˜“è§„æ¨¡ç›¸ä¼¼
            continue
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = min(0.95, 0.5 + len(all_addresses) * 0.05 + similar_size_ratio * 0.2)
        
        # åˆ›å»ºé›†ç¾¤
        cluster_id = hashlib.md5(
            f"{token_id}_{time_window}_{side}".encode()
        ).hexdigest()[:12]
        
        cluster = SybilCluster(
            cluster_id=cluster_id,
            addresses=list(all_addresses),
            market_id=token_id,
            side=side,
            trade_count=len(group),
            total_volume=group['volume'].sum(),
            win_rate=0.0,  # éœ€è¦åç»­è®¡ç®—
            time_window_seconds=time_window_seconds,
            confidence=confidence
        )
        clusters.append(cluster)
    
    # åˆå¹¶ç›¸é‚»æ—¶é—´çª—å£çš„ç›¸ä¼¼é›†ç¾¤
    merged_clusters = _merge_adjacent_clusters(clusters)
    
    logger.info(f"âœ… å¥³å·«é›†ç¾¤æ£€æµ‹å®Œæˆ: å‘ç° {len(merged_clusters)} ä¸ªé›†ç¾¤")
    return merged_clusters


def _merge_adjacent_clusters(clusters: List[SybilCluster]) -> List[SybilCluster]:
    """åˆå¹¶ç›¸é‚»æ—¶é—´çª—å£çš„ç›¸ä¼¼é›†ç¾¤"""
    if not clusters:
        return []
    
    # æŒ‰å¸‚åœºå’Œæ–¹å‘åˆ†ç»„
    by_market_side = defaultdict(list)
    for cluster in clusters:
        key = (cluster.market_id, cluster.side)
        by_market_side[key].append(cluster)
    
    merged = []
    for (market_id, side), market_clusters in by_market_side.items():
        # æŒ‰åœ°å€é‡å åˆå¹¶
        while True:
            merged_any = False
            i = 0
            while i < len(market_clusters):
                j = i + 1
                while j < len(market_clusters):
                    # æ£€æŸ¥åœ°å€é‡å 
                    addr_i = set(market_clusters[i].addresses)
                    addr_j = set(market_clusters[j].addresses)
                    overlap = len(addr_i & addr_j) / max(len(addr_i), len(addr_j))
                    
                    if overlap > 0.5:  # è¶…è¿‡50%é‡å ï¼Œåˆå¹¶
                        # åˆå¹¶é›†ç¾¤
                        market_clusters[i] = SybilCluster(
                            cluster_id=market_clusters[i].cluster_id,
                            addresses=list(addr_i | addr_j),
                            market_id=market_id,
                            side=side,
                            trade_count=market_clusters[i].trade_count + market_clusters[j].trade_count,
                            total_volume=market_clusters[i].total_volume + market_clusters[j].total_volume,
                            win_rate=0.0,
                            time_window_seconds=market_clusters[i].time_window_seconds,
                            confidence=max(market_clusters[i].confidence, market_clusters[j].confidence)
                        )
                        market_clusters.pop(j)
                        merged_any = True
                    else:
                        j += 1
                i += 1
            
            if not merged_any:
                break
        
        merged.extend(market_clusters)
    
    return merged


def sybil_clusters_to_evidence(clusters: List[SybilCluster]) -> List[WashTradeEvidence]:
    """å°†å¥³å·«é›†ç¾¤è½¬æ¢ä¸ºè¯æ®æ ¼å¼"""
    evidence_list = []
    
    for cluster in clusters:
        evidence = WashTradeEvidence(
            evidence_type="SYBIL_CLUSTER",
            tx_hash="",  # å¤šç¬”äº¤æ˜“
            addresses=cluster.addresses,
            confidence=cluster.confidence,
            volume=cluster.total_volume,
            details={
                'cluster_id': cluster.cluster_id,
                'market_id': cluster.market_id,
                'side': cluster.side,
                'trade_count': cluster.trade_count,
                'address_count': len(cluster.addresses),
                'time_window_seconds': cluster.time_window_seconds,
            }
        )
        evidence_list.append(evidence)
    
    return evidence_list


# ============================================================================
# 6. ç»¼åˆå¸‚åœºå¥åº·è¯„ä¼°
# ============================================================================

class MarketForensicsReport:
    """
    å¸‚åœºå–è¯æŠ¥å‘Šç”Ÿæˆå™¨
    
    æ•´åˆæ‰€æœ‰æ£€æµ‹å™¨ï¼Œè¾“å‡ºå¸‚åœºå¥åº·è¯„åˆ†å’Œè¯æ®åˆ—è¡¨
    """
    
    def __init__(self):
        self.detectors_enabled = {
            'self_trades': True,
            'circular_trades': HAS_NETWORKX,
            'atomic_wash': True,
            'volume_spikes': True,
            'sybil_clusters': True,
            'new_wallet_insider': True,
            'high_win_rate': True,
            'gas_anomaly': True,
        }
    
    def run_full_analysis(
        self,
        trades_df: pd.DataFrame,
        logs_df: Optional[pd.DataFrame] = None,
        news_timestamps: Optional[List[datetime]] = None
    ) -> Dict[str, any]:
        """
        è¿è¡Œå®Œæ•´çš„å¸‚åœºåˆ†æ
        
        Args:
            trades_df: äº¤æ˜“ DataFrame
            logs_df: æ—¥å¿— DataFrameï¼ˆå¯é€‰ï¼‰
            news_timestamps: æ–°é—»æ—¶é—´æˆ³ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            åˆ†ææŠ¥å‘Šå­—å…¸
        """
        logger.info("ğŸš€ å¼€å§‹å®Œæ•´å¸‚åœºå–è¯åˆ†æ...")
        
        all_evidence: List[WashTradeEvidence] = []
        detector_results = {}
        
        # 1. è‡ªäº¤æ˜“æ£€æµ‹
        if self.detectors_enabled['self_trades']:
            self_trade_evidence = detect_self_trades(trades_df)
            all_evidence.extend(self_trade_evidence)
            detector_results['self_trades'] = {
                'count': len(self_trade_evidence),
                'volume': sum(e.volume for e in self_trade_evidence)
            }
        
        # 2. å¾ªç¯äº¤æ˜“æ£€æµ‹
        if self.detectors_enabled['circular_trades']:
            circular_paths = detect_circular_trades(trades_df)
            circular_evidence = circular_paths_to_evidence(circular_paths)
            all_evidence.extend(circular_evidence)
            detector_results['circular_trades'] = {
                'count': len(circular_paths),
                'volume': sum(p.total_volume for p in circular_paths)
            }
        
        # 3. åŸå­åŒ–åˆ·é‡æ£€æµ‹
        if self.detectors_enabled['atomic_wash']:
            atomic_evidence = detect_atomic_wash_patterns(trades_df, logs_df)
            all_evidence.extend(atomic_evidence)
            detector_results['atomic_wash'] = {
                'count': len(atomic_evidence),
                'volume': sum(e.volume for e in atomic_evidence)
            }
        
        # 4. äº¤æ˜“é‡å¼‚å¸¸æ£€æµ‹
        if self.detectors_enabled['volume_spikes']:
            volume_spikes = detect_volume_spikes(trades_df, news_timestamps=news_timestamps)
            spike_evidence = volume_spikes_to_evidence(volume_spikes)
            all_evidence.extend(spike_evidence)
            detector_results['volume_spikes'] = {
                'count': len(volume_spikes),
                'volume': sum(s.spike_volume for s in volume_spikes)
            }
        
        # 5. å¥³å·«é›†ç¾¤æ£€æµ‹
        if self.detectors_enabled['sybil_clusters']:
            sybil_clusters = detect_coordinated_clusters(trades_df)
            cluster_evidence = sybil_clusters_to_evidence(sybil_clusters)
            all_evidence.extend(cluster_evidence)
            detector_results['sybil_clusters'] = {
                'count': len(sybil_clusters),
                'volume': sum(c.total_volume for c in sybil_clusters),
                'addresses': sum(len(c.addresses) for c in sybil_clusters)
            }
        
        # 6-8. å¯¼å…¥ä¹‹å‰çš„æ£€æµ‹å™¨
        try:
            from .analyzer import (
                detect_new_wallet_insider,
                get_flagged_traders,
                detect_gas_anomalies
            )
            
            if self.detectors_enabled['new_wallet_insider']:
                insider_flags = detect_new_wallet_insider(trades_df)
                for f in insider_flags:
                    all_evidence.append(WashTradeEvidence(
                        evidence_type="NEW_WALLET_INSIDER",
                        tx_hash=f.tx_hash,
                        addresses=[f.wallet_address],
                        confidence=f.confidence,
                        volume=f.details.get('trade_size', 0),
                        details=f.details
                    ))
                detector_results['new_wallet_insider'] = {
                    'count': len(insider_flags)
                }
            
            if self.detectors_enabled['high_win_rate']:
                winrate_flags = get_flagged_traders(trades_df)
                for f in winrate_flags:
                    all_evidence.append(WashTradeEvidence(
                        evidence_type="HIGH_WIN_RATE",
                        tx_hash=f.tx_hash,
                        addresses=[f.wallet_address],
                        confidence=f.confidence,
                        volume=0,
                        details=f.details
                    ))
                detector_results['high_win_rate'] = {
                    'count': len(winrate_flags)
                }
            
            if self.detectors_enabled['gas_anomaly']:
                gas_flags = detect_gas_anomalies(trades_df)
                for f in gas_flags:
                    all_evidence.append(WashTradeEvidence(
                        evidence_type="GAS_ANOMALY",
                        tx_hash=f.tx_hash,
                        addresses=[f.wallet_address],
                        confidence=f.confidence,
                        volume=f.details.get('size', 0),
                        details=f.details
                    ))
                detector_results['gas_anomaly'] = {
                    'count': len(gas_flags)
                }
                
        except ImportError as e:
            logger.warning(f"æ— æ³•å¯¼å…¥åŸºç¡€åˆ†æå™¨: {e}")
        
        # è®¡ç®—å¸‚åœºå¥åº·è¯„åˆ†
        health_score = self._calculate_health_score(trades_df, all_evidence)
        risk_level = self._get_risk_level(health_score)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            'health_score': health_score,
            'risk_level': risk_level,
            'total_trades': len(trades_df),
            'total_volume': trades_df['volume'].sum() if not trades_df.empty else 0,
            'evidence_count': len(all_evidence),
            'evidence_by_type': self._group_evidence_by_type(all_evidence),
            'detector_results': detector_results,
            'top_evidence': self._get_top_evidence(all_evidence, limit=20),
            'suspicious_addresses': self._get_suspicious_addresses(all_evidence),
            'timestamp': datetime.utcnow().isoformat(),
        }
        
        logger.info(f"âœ… å¸‚åœºå–è¯åˆ†æå®Œæˆ: å¥åº·è¯„åˆ† {health_score:.1f}/100, é£é™©ç­‰çº§ {risk_level}")
        
        return report
    
    def _calculate_health_score(
        self,
        trades_df: pd.DataFrame,
        evidence: List[WashTradeEvidence]
    ) -> float:
        """è®¡ç®—å¸‚åœºå¥åº·è¯„åˆ† (0-100)"""
        if trades_df.empty:
            return 100.0
        
        total_trades = len(trades_df)
        total_volume = trades_df['volume'].sum()
        
        # åŸºç¡€åˆ†æ•° 100
        score = 100.0
        
        # æ ¹æ®è¯æ®æ‰£åˆ†
        for e in evidence:
            penalty = 0
            
            if e.evidence_type == "SELF_TRADE_DIRECT":
                penalty = 5 * e.confidence
            elif e.evidence_type == "SELF_TRADE_COORDINATED":
                penalty = 3 * e.confidence
            elif e.evidence_type == "CIRCULAR_TRADE":
                penalty = 4 * e.confidence
            elif e.evidence_type == "ATOMIC_WASH":
                penalty = 6 * e.confidence
            elif e.evidence_type == "VOLUME_SPIKE":
                penalty = 2 * e.confidence if not e.details.get('is_correlated_with_event') else 0.5
            elif e.evidence_type == "SYBIL_CLUSTER":
                penalty = 5 * e.confidence
            elif e.evidence_type == "NEW_WALLET_INSIDER":
                penalty = 3 * e.confidence
            elif e.evidence_type == "HIGH_WIN_RATE":
                penalty = 2 * e.confidence
            elif e.evidence_type == "GAS_ANOMALY":
                penalty = 1 * e.confidence
            
            # æ ¹æ®äº¤æ˜“é‡æ¯”ä¾‹è°ƒæ•´æƒ©ç½š
            if total_volume > 0 and e.volume > 0:
                volume_ratio = e.volume / total_volume
                penalty *= (1 + volume_ratio * 2)
            
            score -= penalty
        
        return max(0, min(100, score))
    
    def _get_risk_level(self, score: float) -> str:
        """æ ¹æ®å¥åº·è¯„åˆ†ç¡®å®šé£é™©ç­‰çº§"""
        if score >= 80:
            return "LOW"
        elif score >= 60:
            return "MEDIUM"
        elif score >= 40:
            return "HIGH"
        else:
            return "CRITICAL"
    
    def _group_evidence_by_type(
        self,
        evidence: List[WashTradeEvidence]
    ) -> Dict[str, int]:
        """æŒ‰ç±»å‹åˆ†ç»„è¯æ®"""
        by_type = defaultdict(int)
        for e in evidence:
            by_type[e.evidence_type] += 1
        return dict(by_type)
    
    def _get_top_evidence(
        self,
        evidence: List[WashTradeEvidence],
        limit: int = 20
    ) -> List[Dict]:
        """è·å–ç½®ä¿¡åº¦æœ€é«˜çš„è¯æ®"""
        sorted_evidence = sorted(evidence, key=lambda x: x.confidence, reverse=True)
        
        return [
            {
                'type': e.evidence_type,
                'tx_hash': e.tx_hash,
                'addresses': e.addresses[:5],  # æœ€å¤š5ä¸ªåœ°å€
                'confidence': e.confidence,
                'volume': e.volume,
                'details': e.details,
            }
            for e in sorted_evidence[:limit]
        ]
    
    def _get_suspicious_addresses(
        self,
        evidence: List[WashTradeEvidence]
    ) -> Dict[str, Dict]:
        """è·å–å¯ç–‘åœ°å€æ±‡æ€»"""
        address_scores = defaultdict(lambda: {'count': 0, 'total_confidence': 0, 'types': set()})
        
        for e in evidence:
            for addr in e.addresses:
                addr_lower = addr.lower()
                address_scores[addr_lower]['count'] += 1
                address_scores[addr_lower]['total_confidence'] += e.confidence
                address_scores[addr_lower]['types'].add(e.evidence_type)
        
        # è½¬æ¢æ ¼å¼å¹¶æ’åº
        result = {}
        for addr, data in address_scores.items():
            avg_confidence = data['total_confidence'] / data['count'] if data['count'] > 0 else 0
            result[addr] = {
                'evidence_count': data['count'],
                'avg_confidence': avg_confidence,
                'evidence_types': list(data['types']),
                'risk_score': min(100, data['count'] * 10 + avg_confidence * 20)
            }
        
        # æŒ‰é£é™©åˆ†æ•°æ’åºï¼Œè¿”å›å‰50
        sorted_addresses = sorted(result.items(), key=lambda x: x[1]['risk_score'], reverse=True)
        return dict(sorted_addresses[:50])


# ============================================================================
# ä¾¿æ·å‡½æ•°
# ============================================================================

def run_market_forensics(
    start_time: Optional[datetime] = None,
    end_time: Optional[datetime] = None,
    limit: int = 50000
) -> Dict:
    """
    è¿è¡Œå®Œæ•´å¸‚åœºå–è¯åˆ†æçš„ä¾¿æ·å‡½æ•°
    
    Args:
        start_time: å¼€å§‹æ—¶é—´
        end_time: ç»“æŸæ—¶é—´
        limit: æœ€å¤§äº¤æ˜“æ•°
    
    Returns:
        åˆ†ææŠ¥å‘Š
    """
    from .analyzer import load_trades_df
    
    trades_df = load_trades_df(start_time, end_time, limit)
    
    if trades_df.empty:
        return {
            'health_score': 100,
            'risk_level': 'LOW',
            'total_trades': 0,
            'evidence_count': 0,
            'message': 'No trades to analyze'
        }
    
    reporter = MarketForensicsReport()
    return reporter.run_full_analysis(trades_df)
